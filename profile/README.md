# LLM-as-an-Interviewer ðŸŽ¤ðŸ“„
This is the official GitHub repository for [LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation](https://arxiv.org/abs/2412.10424).

LLM-as-an-Interviewer is an evaluation framework that assesses the capabilities of LLMs through an interview-style process. In this approach, the LLM acting as the interviewer evaluates other LLMs by providing feedback and asking follow-up questions, enabling a more comprehensive assessment of their capabilities.

Our framework includes a flexible pipeline that can be easily adapted to various tasks by incorporating a customized evaluation rubric.

Here's an improved version of your README content for clarity and professionalism:

---

### Code for Paper Replication
Access the code used to replicate the results discussed in our paper. [github](https://github.com/interview-eval/interview-eval-paper)

---

### Code for the Framework
Explore the framework implementation. [github](https://github.com/interview-eval/interview-eval) 

---

### PyPI
The framework is also available as a Python package on PyPI. Install it using:  
```bash
pip install interview-eval
```
[Visit the PyPI page](https://pypi.org/project/interview-eval/) 

---
